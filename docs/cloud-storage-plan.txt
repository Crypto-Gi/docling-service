Enable 
_save_images()
 in 
app/main.py
 to upload generated PNGs to cloud storage, replace local paths with public URLs, and have Markdown reference those URLs.

Plan
Assess prerequisites: cloud provider choice, credentials, 
Settings
 additions, local vs cloud toggle.
Implement upload workflow: update 
_save_images()
, add helper to upload, persist URLs in image map, ensure cleanup/error handling.
Validate & operate: logging, retries, tests, cost/security, rollout steps.
Detailing the Plan
1. Prerequisites & Configuration
Cloud provider selection: Decide service (e.g., AWS S3, Google Cloud Storage, Azure Blob, Cloudflare R2). Each requires SDK and credentials.
Dependency management: Add provider SDK to requirements.txt (e.g., boto3 for S3) and rebuild images.
Environment variables: Extend 
app/config.py
 to read:
DOCLING_CLOUD_UPLOAD_ENABLED (bool toggle)
Storage-specific settings: bucket name, region, base URL, optional CDN prefix.
Authentication keys (or rely on instance profiles/managed identities).
Directory policy: Determine whether to keep local copies. If uploading to cloud, optionally delete local PNGs to save space; ensure Markdown references cloud URLs.
Security: Store secrets in 
.env
 or platform secret manager; never hardcode. Limit IAM permissions to needed actions (upload, set ACL).
2. Code Changes
2.1 Settings enhancements
Update 
Settings
 in 
app/config.py
:
python
class Settings:
    cloud_upload_enabled: bool = os.getenv("DOCLING_CLOUD_UPLOAD_ENABLED", "false").lower() in {"1","true","yes","on"}
    cloud_bucket: str | None = os.getenv("DOCLING_CLOUD_BUCKET")
    cloud_region: str | None = os.getenv("DOCLING_CLOUD_REGION")
    cloud_base_url: str | None = os.getenv("DOCLING_CLOUD_BASE_URL")  # e.g., https://cdn.example.com
Add derived properties if needed (e.g., computed prefix).
2.2 Upload helper
Create app/cloud_uploader.py (or integrate into existing module):
python
class CloudUploader:
    def __init__(self, settings: Settings):
        self.enabled = settings.cloud_upload_enabled
        # initialize SDK client (e.g., boto3 client)
    
    def upload(self, local_path: Path, key: str) -> str:
        # upload file to bucket/key, return public URL
Support asynchronous calls via asyncio.to_thread.
2.3 Modify 
_save_images()
Signature: async def _save_images(..., uploader: CloudUploader | None):
After saving locally, if uploader.enabled:
Compute object key, e.g., f"{task_id}/picture-{counter}.png".
Call await asyncio.to_thread(uploader.upload, image_filename, key).
Capture returned URL.
Optionally delete local file if not needed (image_filename.unlink()).
Store URL (or relative local path fallback) in image_map[element.self_ref].
Ensure URLs stored in image_map replace local Path objects; adjust 
_update_image_uris()
 to accept str for URIs (cloud URLs may be HTTPS).
2.4 Maintain local fallback
If upload fails, either:
Retry with exponential backoff.
Fallback to local path to avoid losing reference; log error; maybe mark task as failed depending on strictness.
Consider feature flag: run uploads only when enabled.
2.5 Update 
TaskManager
 initialization
Instantiate CloudUploader and pass to 
_save_images()
 inside 
_execute()
.
2.6 Markdown references
_update_image_uris()
 should set element.image.uri to the final string (cloud URL or relative local path). Ensure 
MarkdownDocSerializer
 emits these.
3. Validation, Security, Operations
Testing:
Unit test cloud uploader (mocking provider SDK).
Integration test: run conversion with cloud_upload_enabled=True using mock/minio local S3; verify Markdown contains HTTPS URLs and cloud directory has files.
Logging & monitoring:
Log uploads (success/failure) with task ID for traceability.
Add metrics (optional) for upload latency or failures.
Error handling:
Retry transient errors; limit attempts; escalate by failing task after repeated errors.
Ensure _cleanup_lock processes still honor storage limits even if local images removed (images directory may remain empty when uploads on).
Security:
Use least-privilege IAM policies.
Do not expose signed URLs publicly unless desired; consider presigned URLs with expiration if private.
Sanitize file names (already controlled via counters). Optionally include hash to prevent collisions.
Cost considerations:
Monitor egress costs if Markdown consumers download often; consider CDN fronting the bucket.
Deployment:
Add env vars to 
docker-compose.yml
/Kubernetes manifests.
Ensure container has credentials (env, mounted secrets, IAM role).
Document configuration in README/ops runbook.
Rollback plan:
Keep local path fallback so disabling cloud flag reverts to previous behavior without code changes.
Next Steps
Gather requirements: choose provider, confirm bucket naming, define public URL pattern.
Implement settings & uploader: add new module and dependencies.
Update image pipeline: integrate upload flow, adjust URI handling.
Test & document: run local/integration tests, update docs/README.
Roll out: deploy with new env vars, monitor logs for upload health.
Summary of Code Changes
No code changes performed; provided implementation plan.